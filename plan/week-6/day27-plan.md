# Day 27 - Audio Integration Testing & Mixing

**Date:** November 22-23, 2025 (Revised)
**Goal:** Verify each audio source works independently via ScreenCaptureKit (system + mic), then implement mixing
**Status:** ‚úÖ COMPLETE - Both Phases Done (with known quality limitations)

---

## Overview - REVISED APPROACH

ScreenCaptureKit on macOS 15+ exposes separate outputs for system audio (`.audio`) and microphone (`.microphone`). It does not auto-mix. We validate each source, then mix in-app:

**Phase 1: Individual Source Testing (Priority)**
1. Wire `SCStream` to emit `.audio` (system) and `.microphone` buffers.
2. Test system audio recording ONLY (disable microphone flag).
3. Test microphone recording ONLY (disable system audio flag).
4. Confirm both sources produce valid audio in final video.

**Phase 2: Audio Mixing (After Phase 1 confirmed working)**
1. Request both outputs from SCStream (macOS 15+).
2. Implement lightweight in-app mixing (up-mix mic mono, sum to system stereo) with format guard.
3. Add volume controls for each source.
4. Add A/V sync monitoring.

**Current State Analysis:**
- ‚úÖ Video capture working (ScreenCaptureEngine)
- ‚úÖ System audio capture implemented via ScreenCaptureKit (Day 25)
- ‚úÖ Microphone capture available via ScreenCaptureKit (macOS 15+) but not yet wired
- ‚ö†Ô∏è System audio recording NOT yet tested end-to-end
- ‚ö†Ô∏è Microphone recording NOT yet tested end-to-end
- ‚ùå No mutually exclusive toggle enforcement yet (for Phase 1 testing)
- ‚ùå No audio mixing implemented
- ‚ùå A/V sync not verified

**Phase 1 Target State:**
- ‚úÖ Users can enable system audio OR microphone (mutually exclusive)
- ‚úÖ System audio only recording produces valid audio output
- ‚úÖ Microphone only recording produces valid audio output
- ‚úÖ Both tested with actual playback verification

**Phase 2 Target State:**
- ‚úÖ System audio + microphone mixed into SINGLE audio track
- ‚ö†Ô∏è Volume controls not implemented (fixed 1:1 mix ratio)
- ‚úÖ Real-time audio mixing before encoding
- ‚úÖ Audio/video sync working (no drift detected)

**Final Results (November 23, 2025):**
- ‚úÖ Audio mixing implemented using SimpleMixer (interleaved Float32 output)
- ‚úÖ Supports all device formats: Int16, Int32, Float32, Float64
- ‚úÖ Handles sample rates: 8kHz-192kHz (tested: 16kHz, 44.1kHz, 48kHz)
- ‚úÖ Device switching mid-recording works (clears cached buffers)
- ‚úÖ Format locking prevents encoder errors (-12737, -11800)
- ‚úÖ Thread-safe with serial dispatch queue
- ‚úÖ AVAudioConverter for professional quality resampling
- ‚ö†Ô∏è Voice quality not optimal (acceptable for MVP)
- ‚ö†Ô∏è Mic-only speech delayed until system buffer arrives
- ‚ö†Ô∏è System audio always converted (small performance overhead)

**Documentation:**
- Created comprehensive learnings doc: `docs/audio-mixing-learnings.md`
- 500+ lines covering all pitfalls, solutions, and best practices

---

## Phase 1: Individual Source Testing

### Implementation Tasks

**Task 1: Wire ScreenCaptureKit microphone capture (45 min)**

**Goal:** Use ScreenCaptureKit for microphone instead of AVAudioEngine.

**Files to modify:**
- `MyRec/Services/Recording/ScreenCaptureEngine.swift`
- `MyRec/Services/AudioCaptureEngine.swift`

**Changes:**
1. Request `.audio` and `.microphone` outputs on macOS 15+ (`SCStream.addStreamOutput`).
2. Route both outputs into the capture engine for monitoring/encoding.
3. Remove dependency on AVAudioEngine tap for mic capture; keep level metering from incoming buffers.

**Task 2: Make Audio Toggles Mutually Exclusive for Phase 1 (20 min)**

**Goal:** Ensure only ONE audio source is enabled at a time while validating SCK capture.

**File to modify:**
- `MyRec/Views/Settings/SettingsBarView.swift`

**Changes:**
1. Add `onChange` handlers to disable the other source when one is turned on.
2. Log which source is active (system vs mic).

**Task 3: Test System Audio Recording (30 min)**

**Test procedure:**
1. Enable system audio toggle ONLY
2. Disable microphone toggle
3. Start recording (play music/video in background)
4. Record for 30 seconds
5. Stop recording
6. Open video file and verify audio is present
7. Check audio sync with video

**Success criteria:**
- [ ] Video file contains audio track
- [ ] Audio is audible and clear
- [ ] Audio matches what was playing during recording
- [ ] No crashes or errors

**Task 4: Test Microphone Recording (30 min)**

**Test procedure:**
1. Disable system audio toggle
2. Enable microphone toggle ONLY
3. Grant microphone permission if needed
4. Start recording (speak into microphone)
5. Record for 30 seconds
6. Stop recording
7. Open video file and verify microphone audio is present
8. Check audio quality

**Success criteria:**
- [ ] Video file contains audio track
- [ ] Microphone audio is audible and clear
- [ ] Voice is recognizable
- [ ] No crashes or errors

**Task 5: Debug and Fix Issues (60 min)**

Based on test results from Tasks 3 & 4:
- Fix any audio encoding issues
- Fix any synchronization problems
- Fix any crashes or errors
- Verify both sources work reliably

### Solution: Add Mixing Layer

```swift
// NEW: Buffer queue approach
class AudioCaptureEngine {
    private var systemAudioBuffers: [CMSampleBuffer] = []
    private var microphoneBuffers: [CMSampleBuffer] = []
    private let bufferLock = NSLock()

    // NEW: Volume controls
    var systemVolume: Float = 1.0  // 0.0 to 1.0
    var microphoneVolume: Float = 1.0  // 0.0 to 1.0

    // When system audio arrives:
    func processSampleBuffer(_ sampleBuffer: CMSampleBuffer) {
        bufferLock.lock()
        systemAudioBuffers.append(sampleBuffer)
        tryMixAndWrite()
        bufferLock.unlock()
    }

    // When microphone audio arrives from ScreenCaptureKit:
    func processMicrophoneBuffer(_ sampleBuffer: CMSampleBuffer) {
        bufferLock.lock()
        microphoneBuffers.append(sampleBuffer)
        tryMixAndWrite()
        bufferLock.unlock()
    }

    // Mix and write if we have both:
    private func tryMixAndWrite() {
        guard !systemAudioBuffers.isEmpty,
              !microphoneBuffers.isEmpty else { return }

        let sysBuf = systemAudioBuffers.removeFirst()
        let micBuf = microphoneBuffers.removeFirst()

        // Mix PCM data
        let mixed = mixBuffers(sysBuf, micBuf)

        // Write to single asset writer input
        assetWriterInput?.append(mixed)
    }
}
```

### Synchronization Strategy

```
System Audio Buffer ‚Üí Queue ‚Üí
                              ‚Üì [Match timestamps] ‚Üí Mix ‚Üí Write to AssetWriter
Microphone Buffer  ‚Üí Queue ‚Üí
                              ‚Üë
                         Monitor drift
                         Log if > 50ms
```

**Phase 1 (Day 27):** Monitoring only - log drift, don't correct
**Phase 2 (Future):** Add correction if needed based on test results

---

## Phase 1 Implementation - COMPLETED ‚úÖ

### Task 1: Wire ScreenCaptureKit Microphone Capture - DONE ‚úÖ

**Completed:**
- ‚úÖ Updated `startCapture()` to accept `withMicrophone` parameter (macOS 15+ only)
- ‚úÖ Added `.microphone` case handler in stream output delegate
- ‚úÖ Registered microphone stream output with ScreenCaptureKit (15+)
- ‚úÖ Added `processMicrophoneBuffer()` to AudioCaptureEngine
- ‚úÖ Implemented microphone level monitoring from CMSampleBuffer
- ‚úÖ Updated AppDelegate to pass microphone flag from settings

**Files Modified:**
- `MyRec/Services/Recording/ScreenCaptureEngine.swift` - Added microphone support (15+)
- `MyRec/Services/AudioCaptureEngine.swift` - Added microphone buffer processing
- `MyRec/AppDelegate.swift` - Pass microphone flag to capture engine

### Task 2: Make Audio Toggles Mutually Exclusive - DONE ‚úÖ

**Completed:**
- ‚úÖ Added `onChange` handler for system audio toggle
- ‚úÖ Added `onChange` handler for microphone toggle
- ‚úÖ When system audio enabled ‚Üí disable microphone
- ‚úÖ When microphone enabled ‚Üí disable system audio
- ‚úÖ Added console logging for Phase 1 mode tracking

**Files Modified:**
- `MyRec/Views/Settings/SettingsBarView.swift` - Added mutually exclusive logic

**Build Status:** ‚úÖ BUILD SUCCEEDED (no errors)

---

## Revised Implementation Tasks (Simplified)

### Task 1: Add Audio Mixing to AudioCaptureEngine (90 min)

**File:** `MyRec/Services/AudioCaptureEngine.swift` (modify existing)

**Purpose:** Add buffer queuing and mixing logic

**Changes to make:**

```swift
// ADD: New properties
public class AudioCaptureEngine: NSObject, ObservableObject {
    // Mixing properties
    @Published var systemVolume: Float = 1.0
    @Published var microphoneVolume: Float = 1.0
    @Published var isMixingEnabled = false

    private var systemAudioBuffers: [CMSampleBuffer] = []
    private var microphoneBuffers: [CMSampleBuffer] = []
    private let bufferLock = NSLock()
    private var syncMonitor = SyncMonitor()

    // Mixing mode: when both system audio AND microphone enabled
    var shouldMixAudio: Bool {
        // Determined by RecordingManager based on which sources are enabled
        return isMixingEnabled
    }
}

// MODIFY: processSampleBuffer
func processSampleBuffer(_ sampleBuffer: CMSampleBuffer) {
    if shouldMixAudio {
        // Queue for mixing
        bufferLock.lock()
        defer { bufferLock.unlock() }
        systemAudioBuffers.append(sampleBuffer)
        tryMixAndWrite()
    } else {
        // Write directly (system audio only mode)
        guard let input = assetWriterInput,
              input.isReadyForMoreMediaData else { return }
        updateAudioLevel(from: sampleBuffer)
        audioQueue.async { input.append(sampleBuffer) }
    }
}

// MODIFY: processMicrophoneBuffer (ScreenCaptureKit microphone output)
func processMicrophoneBuffer(_ sampleBuffer: CMSampleBuffer) {
    updateAudioLevel(from: sampleBuffer)

    guard isCapturing else { return }

    if shouldMixAudio {
        bufferLock.lock()
        defer { bufferLock.unlock() }
        microphoneBuffers.append(sampleBuffer)
        tryMixAndWrite()
    } else {
        // Write directly (microphone only mode)
        assetWriterInput?.append(sampleBuffer)
    }
}

// NEW: Mix and write
private func tryMixAndWrite() {
    guard !systemAudioBuffers.isEmpty,
          !microphoneBuffers.isEmpty else { return }

    let systemBuffer = systemAudioBuffers.removeFirst()
    let micBuffer = microphoneBuffers.removeFirst()

    // Monitor sync
    let sysPTS = CMSampleBufferGetPresentationTimeStamp(systemBuffer)
    let micPTS = CMSampleBufferGetPresentationTimeStamp(micBuffer)
    syncMonitor.recordSyncPoint(videoTime: sysPTS, audioTime: micPTS)

    // Mix buffers
    if let mixedBuffer = mixBuffers(system: systemBuffer, microphone: micBuffer) {
        guard let input = assetWriterInput,
              input.isReadyForMoreMediaData else { return }

        audioQueue.async {
            input.append(mixedBuffer)
        }
    }
}

// NEW: Actual mixing logic
private func mixBuffers(system: CMSampleBuffer, microphone: CMSampleBuffer) -> CMSampleBuffer? {
    // Extract PCM data from both buffers
    guard let systemPCM = extractPCMData(from: system),
          let micPCM = extractPCMData(from: microphone) else {
        return nil
    }

    // Use shorter length
    let length = min(systemPCM.count, micPCM.count)
    var mixedData = [Float](repeating: 0, count: length)

    // Mix with volume controls
    for i in 0..<length {
        let systemSample = systemPCM[i] * systemVolume
        let micSample = micPCM[i] * microphoneVolume
        mixedData[i] = systemSample + micSample

        // Soft clipping to prevent distortion
        if mixedData[i] > 1.0 {
            mixedData[i] = tanh(mixedData[i])
        } else if mixedData[i] < -1.0 {
            mixedData[i] = tanh(mixedData[i])
        }
    }

    // Convert back to CMSampleBuffer using system buffer's timestamp
    return createCMSampleBuffer(
        from: mixedData,
        timestamp: CMSampleBufferGetPresentationTimeStamp(system),
        sampleRate: 48000
    )
}

// NEW: Extract PCM data from CMSampleBuffer
private func extractPCMData(from sampleBuffer: CMSampleBuffer) -> [Float]? {
    guard let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) else {
        return nil
    }

    var length = 0
    var dataPointer: UnsafeMutablePointer<Int8>?

    let status = CMBlockBufferGetDataPointer(
        blockBuffer,
        atOffset: 0,
        lengthAtOffsetOut: &length,
        totalLengthOut: nil,
        dataPointerOut: &dataPointer
    )

    guard status == noErr, let data = dataPointer else {
        return nil
    }

    // Assuming Float32 PCM format
    let samples = UnsafeRawPointer(data).assumingMemoryBound(to: Float.self)
    let count = length / MemoryLayout<Float>.size

    return Array(UnsafeBufferPointer(start: samples, count: count))
}

// NEW: Create CMSampleBuffer from PCM data
private func createCMSampleBuffer(
    from pcmData: [Float],
    timestamp: CMTime,
    sampleRate: Double
) -> CMSampleBuffer? {
    // Implementation using AVAudioFormat and CMSampleBufferCreate
    // (Similar to existing convertToCMSampleBuffer method)
    // Details to be implemented during Task 1
    return nil  // Placeholder
}
```

**Subtasks:**
1. Add mixing properties and buffer queues
2. Modify processSampleBuffer to support mixing mode
3. Modify processMicrophoneBuffer to support mixing mode
4. Implement tryMixAndWrite logic
5. Implement mixBuffers with volume controls
6. Implement extractPCMData helper
7. Implement createCMSampleBuffer helper
8. Test with both audio sources enabled

---

### Task 2: Create SyncMonitor Helper (30 min)

**File:** `MyRec/Services/SyncMonitor.swift` (new)

**Purpose:** Monitor A/V sync drift and log warnings

```swift
import Foundation
import CoreMedia

class SyncMonitor {
    private var syncPoints: [(videoTime: CMTime, audioTime: CMTime, drift: Double)] = []
    private let maxDriftMs: Double = 50.0

    func recordSyncPoint(videoTime: CMTime, audioTime: CMTime) {
        let videoSeconds = CMTimeGetSeconds(videoTime)
        let audioSeconds = CMTimeGetSeconds(audioTime)
        let driftMs = (videoSeconds - audioSeconds) * 1000

        syncPoints.append((videoTime, audioTime, driftMs))

        // Keep last 100 points
        if syncPoints.count > 100 {
            syncPoints.removeFirst()
        }

        // Log significant drift
        if abs(driftMs) > maxDriftMs {
            print("‚ö†Ô∏è SYNC DRIFT: \(String(format: "%.2f", driftMs))ms at \(String(format: "%.2f", videoSeconds))s")
        }
    }

    func getCurrentDrift() -> Double {
        syncPoints.last?.drift ?? 0
    }

    func getAverageDrift() -> Double {
        guard !syncPoints.isEmpty else { return 0 }
        return syncPoints.map { $0.drift }.reduce(0, +) / Double(syncPoints.count)
    }

    func getMaxDrift() -> Double {
        syncPoints.map { abs($0.drift) }.max() ?? 0
    }

    func reset() {
        syncPoints.removeAll()
    }

    func printSyncReport() {
        print("üìä Sync Report:")
        print("   Total samples: \(syncPoints.count)")
        print("   Current drift: \(String(format: "%.2f", getCurrentDrift()))ms")
        print("   Average drift: \(String(format: "%.2f", getAverageDrift()))ms")
        print("   Max drift: \(String(format: "%.2f", getMaxDrift()))ms")
    }
}
```

---

### Task 3: Add Volume Control UI (45 min)

**File:** `MyRec/Views/Settings/SettingsBarView.swift` (modify existing)

**Purpose:** Add sliders for system audio and microphone volume

**Changes:**
```swift
// Add to existing SettingsBarView
VStack {
    // Existing settings...

    if viewModel.systemAudioEnabled || viewModel.microphoneEnabled {
        Divider()

        VStack(spacing: 8) {
            Text("Audio Levels")
                .font(.caption)
                .foregroundColor(.secondary)

            if viewModel.systemAudioEnabled {
                HStack {
                    Image(systemName: "speaker.wave.2")
                        .frame(width: 16)
                    Slider(value: $viewModel.systemVolume, in: 0...1)
                        .frame(width: 100)
                    Text("\(Int(viewModel.systemVolume * 100))%")
                        .font(.caption2)
                        .frame(width: 35)
                }
            }

            if viewModel.microphoneEnabled {
                HStack {
                    Image(systemName: "mic")
                        .frame(width: 16)
                    Slider(value: $viewModel.microphoneVolume, in: 0...1)
                        .frame(width: 100)
                    Text("\(Int(viewModel.microphoneVolume * 100))%")
                        .font(.caption2)
                        .frame(width: 35)
                }
            }
        }
    }
}
```

**Also add to RegionSelectionViewModel:**
```swift
@Published var systemVolume: Float = 1.0
@Published var microphoneVolume: Float = 1.0

// In toggleSystemAudio() and toggleMicrophone()
// Connect these to audioEngine.systemVolume and audioEngine.microphoneVolume
```

---

### Task 4: Write Unit Tests (45 min)

**File:** `Tests/MyRecTests/AudioMixingTests.swift` (new)

**Purpose:** Test audio mixing logic

```swift
import XCTest
@testable import MyRec

final class AudioMixingTests: XCTestCase {
    func testVolumeMixing() {
        // Create mock PCM data
        let systemData: [Float] = [0.5, 0.5, 0.5, 0.5]
        let micData: [Float] = [0.3, 0.3, 0.3, 0.3]

        // Mix with different volumes
        let systemVolume: Float = 1.0
        let micVolume: Float = 0.5

        var mixed = [Float](repeating: 0, count: 4)
        for i in 0..<4 {
            mixed[i] = (systemData[i] * systemVolume) + (micData[i] * micVolume)
        }

        // Expected: 0.5 + (0.3 * 0.5) = 0.65
        XCTAssertEqual(mixed[0], 0.65, accuracy: 0.01)
    }

    func testSyncMonitor() {
        let monitor = SyncMonitor()

        let time1 = CMTime(seconds: 1.000, preferredTimescale: 1000)
        let time2 = CMTime(seconds: 1.001, preferredTimescale: 1000)

        monitor.recordSyncPoint(videoTime: time1, audioTime: time2)

        // Drift should be -1ms (audio ahead by 1ms)
        XCTAssertEqual(monitor.getCurrentDrift(), -1.0, accuracy: 0.1)
    }

    func testSoftClipping() {
        // Test that values > 1.0 are clipped
        let value: Float = 1.5
        let clipped = tanh(value)

        XCTAssertLessThan(clipped, 1.0)
        XCTAssertGreaterThan(clipped, 0.9)  // tanh(1.5) ‚âà 0.905
    }
}
```

---

### Task 5: Integration Testing & Long Recording Test (120 min)

**Manual test scenarios:**

1. **Basic mixing test (10 min):**
   - [ ] Enable system audio only ‚Üí record ‚Üí verify audio present
   - [ ] Enable microphone only ‚Üí record ‚Üí verify audio present
   - [ ] Enable both ‚Üí record ‚Üí verify both audible in output

2. **Volume control test (15 min):**
   - [ ] System at 100%, mic at 50% ‚Üí verify levels
   - [ ] System at 50%, mic at 100% ‚Üí verify levels
   - [ ] System at 0% (muted), mic at 100% ‚Üí verify only mic audible
   - [ ] Both at 50% ‚Üí verify balanced mix

3. **Synchronization test (30 min):**
   - [ ] Record with visible timer on screen
   - [ ] Play audio click/beep every second
   - [ ] Record for 5 minutes
   - [ ] Check sync at 0s, 30s, 1min, 2min, 5min
   - [ ] Measure drift in video player/editor

4. **Long recording test (60 min):**
   - [ ] Record for 30+ minutes
   - [ ] Monitor console for drift warnings
   - [ ] Check memory usage (Activity Monitor)
   - [ ] Verify final sync is within ¬±50ms
   - [ ] Check for buffer overflow/underflow logs

5. **Edge cases (5 min):**
   - [ ] Very loud system audio + quiet mic
   - [ ] Very loud mic + quiet system audio
   - [ ] Both at maximum (check clipping/distortion)

---

---

## Phase 2: Audio Mixing Implementation

**NOTE:** Only proceed to Phase 2 after Phase 1 tests pass!

### Implementation Tasks

**Task 5: Create SyncMonitor Helper (30 min)**
- Create `MyRec/Services/SyncMonitor.swift`
- Implement drift tracking and logging
- Add console reporting

**Task 6: Implement Audio Mixing in AudioCaptureEngine (90 min)**
- Add buffer queues for system audio and microphone
- Implement `mixBuffers()` method to combine PCM data
- Add soft clipping to prevent distortion
- Update `processSampleBuffer()` to queue when mixing mode enabled
- Update `processMicrophoneBuffer()` to queue when mixing mode enabled

**Task 7: Add Volume Controls (45 min)**
- Add `@Published var systemVolume: Float = 1.0` to AudioCaptureEngine
- Add `@Published var microphoneVolume: Float = 1.0` to AudioCaptureEngine
- Add sliders to SettingsBarView
- Connect sliders to AudioCaptureEngine properties

**Task 8: Enable Both Audio Sources & Test Mixing (60 min)**
- Remove mutually exclusive toggle logic
- Allow both audio sources to be enabled simultaneously
- Test mixed audio recording
- Verify both sources audible in final video
- Check for clipping or distortion

---

## Revised Time Breakdown

### Phase 1: Individual Source Testing (Current Focus)
| Task | Estimated | Actual |
|------|-----------|--------|
| Make toggles mutually exclusive | 30 min | - |
| Test system audio recording | 30 min | - |
| Test microphone recording | 30 min | - |
| Debug and fix issues | 60 min | - |
| **Phase 1 Total** | **2.5 hours** | - |

### Phase 2: Audio Mixing (After Phase 1)
| Task | Estimated | Actual |
|------|-----------|--------|
| Create SyncMonitor | 30 min | - |
| Implement audio mixing logic | 90 min | - |
| Add volume controls | 45 min | - |
| Test mixing & verify quality | 60 min | - |
| Write unit tests | 45 min | - |
| **Phase 2 Total** | **~4.5 hours** | - |

**Total Estimated:** ~7 hours (split across 2 sessions if needed)

---

## Success Criteria

### Phase 1 Success Criteria (Must complete first)

**Functional:**
- [ ] Only one audio source can be enabled at a time (mutually exclusive toggles)
- [ ] System audio only recording works and produces audible output
- [ ] Microphone only recording works and produces audible output
- [ ] No crashes during recording or playback

**Quality:**
- [ ] Audio is clearly audible in both test cases
- [ ] Audio sync is acceptable (rough check, no precise measurement yet)
- [ ] No build errors or warnings

**Testing:**
- [ ] System audio test recorded and played back successfully
- [ ] Microphone test recorded and played back successfully
- [ ] Build passes: `swift test` runs without errors

### Phase 2 Success Criteria (After Phase 1 passes)

**Functional:**
- [ ] System audio + microphone mixed into single track
- [ ] Volume controls working (0-100% for each source)
- [ ] Both sources audible in final video
- [ ] No clipping or distortion

**Quality:**
- [ ] A/V sync within ¬±50ms throughout recording
- [ ] Max drift logged and monitored
- [ ] No memory leaks or crashes
- [ ] Zero build errors/warnings

**Testing:**
- [ ] Unit tests pass (swift test)
- [ ] Manual mixing tests pass
- [ ] Both sources audible and balanced in mixed recording

---

## Blockers & Risk Mitigation

**Potential Blockers:**
1. PCM format mismatch between system audio and microphone
   - **Mitigation:** Convert both to Float32 PCM before mixing

2. Buffer timing mismatch
   - **Mitigation:** Use presentation timestamps, queue buffers

3. Excessive drift accumulation
   - **Mitigation:** Log and monitor; implement correction if needed (Phase 2)

4. Memory buildup from buffer queues
   - **Mitigation:** Limit queue size, drop old buffers if needed

---

## Notes for Implementation

**Important:**
- Both processSampleBuffer and processMicrophoneBuffer must check `shouldMixAudio` flag
- When only ONE audio source enabled ‚Üí write directly (no mixing overhead)
- When BOTH enabled ‚Üí queue buffers and mix
- Use system audio timestamp as canonical time (it's from ScreenCaptureKit)
- Log ALL sync drift warnings for analysis

**Testing priority:**
1. Basic mixing (high priority)
2. Volume controls (high priority)
3. Short sync test (high priority)
4. Long recording (medium priority - can be done overnight)

---

## Results (End of Day)

**Status:** ‚úÖ Phase 1 COMPLETE - Both Audio Sources Verified Working

### Phase 1 Results

**Completed:**
- [x] Mutually exclusive toggles implemented
- [x] ScreenCaptureKit microphone wiring complete
- [x] AppDelegate updated to pass audio flags
- [x] Removed AudioCaptureEngine completely
- [x] VideoEncoder updated to write audio directly
- [x] System audio test completed ‚úÖ
- [x] Microphone test completed ‚úÖ
- [x] Both tests verified with playback ‚úÖ

**Phase 1 Metrics:**
- Build status: ‚úÖ BUILD SUCCEEDED
- Implementation time: ~2 hours (including troubleshooting)
- System audio quality: ‚úÖ Working perfectly
- Microphone audio quality: ‚úÖ Working perfectly
- Issues found & fixed:
  1. ‚ùå AudioCaptureEngine was not writing audio to file
  2. ‚úÖ Fixed by removing AudioCaptureEngine and writing directly in VideoEncoder
  3. ‚úÖ Both audio sources now record to MP4 successfully

**Key Architectural Change:**
- **Before:** ScreenCaptureKit ‚Üí ScreenCaptureEngine ‚Üí VideoEncoder ‚Üí AudioCaptureEngine ‚Üí ‚ùå (dropped)
- **After:** ScreenCaptureKit ‚Üí ScreenCaptureEngine ‚Üí VideoEncoder ‚Üí AVAssetWriterInput ‚Üí ‚úÖ MP4 file

### Phase 2 Results (If time permits)

**Completed:**
- [ ] Audio mixing implemented
- [ ] Volume controls functional
- [ ] Mixed recording tested
- [ ] Unit tests passing

**Phase 2 Metrics:**
- Build status: -
- Test pass rate: -
- Max observed drift: -
- Mix quality: -

**Blockers encountered:** -

**Next steps:**
- ‚úÖ Phase 1 complete: Both audio sources working independently
- Phase 2: Audio mixing (system + microphone simultaneously)
  - Requires buffer queuing and PCM mixing
  - Can be deferred to future sprint if needed
- Alternative: Move to Week 7 tasks (pause/resume, camera integration)

**Summary:**
Phase 1 achieved the core goal: verified ScreenCaptureKit can capture both system audio and microphone independently. The application now has working audio recording for both sources using a clean, simplified architecture (ScreenCaptureKit only, no AVAudioEngine complexity).

---

**Last Updated:** November 22, 2025 (Day 27 Phase 1 COMPLETE - Both audio sources working)
